# 

## 1 Summary

## Table of Contents

[TOC]

## 2 Introduction

Wordle 是《纽约时报》推出的一款火遍全球的填字解密游戏。尽管游戏规则简单，寻求解的过程却蕴含着概率、信息熵等深刻的数学知识，游戏本身也因此受到了诸多关注。每天，有大量的玩家在 Twitter 上报告他们的游戏结果，这些结果由推特账号 Wordle Stats 自动整理，并公布于众。

基于其中的一部分数据（从 2022 年 1 月 7 日到 12 月 31 日），本文完成了如下工作：

- 建立每日结果报告数对天数的模型，并据此预测 2023 年 3 月 1 日的结果报告数。
- 分析单词猜测的结果百分比与单词本身的哪些特征有关，同时建立结果百分比对这些特征的模型，并据此预测 2023 年 3 月 1 日的单词 $\tt eerie$ 的结果百分比分布。
- 建立单词难度的评估模型，并据预测结果评估单词 $\tt eerie$ 的难度。
- 对数据做其他分析。

最后，上述所有工作的核心结果将被整理在写给《纽约时报》编辑者的一封信中，从而体现出本文的实践与应用价值。

## 3 Task 1: Reported Result Modeling and Analysis

### 3.1 Model and Predict the Number of Reported Results

根据经验，我们会认为一款游戏的游玩人数（在这里可以视为等同于报告结果数）通常是先增加再减少的——起初，游戏不断地在人群中扩散传播，并且新颖的游戏规则的确捕获了许多人游玩，因此数量会增加，但随着时间的推移，大部分人会对这款游戏逐渐产生厌倦，同时游戏的传播力度会不断下降，受众最终达到饱和，因此数量会减少。设从 2022 年 1 月 7 日开始，每一天的编号依次为 $1, 2, \cdots$，利用给定文件中的数据绘制的报告结果数如图 1 所示（横向数据表示天的编号，纵向数据表示报告结果数，下同）。[^1]

[^1]:给定文件中 2022 年 11 月 30 日的数据异常，已将其删除。

![](fig1.png "图 1")

总体上，结果的趋势与我们的经验估计相吻合。接下来，设图 1 中天数编号 $t$ 与当天报告结果数 $N$ 之间大致满足关系
$$N(t) = \displaystyle\frac{f(t)}{g(t)}.$$

这里选用分式函数的原因与我们的经验分析密切相关，函数 $f(t)$ 与 $g(t)$ 大体上都随着时间 $t$ 的增加单调不降，分子函数 $f(t)$ 描述了游戏不断在人群中扩散，报告结果数理论上应不断增加的情形，但实际上，它受到了描述对游戏厌倦程度的分母函数 $g(t)$ 的约束，且从数学上，$g(t)$ 比 $f(t)$ 更“高阶”，这样才能达到图 1 表现出的 $\displaystyle\lim_{t \to \infty} N(t) = 0$ 的效果。

对函数 $g(t)$ 进行理论分析与构建是较为困难的，我们仅仅知道它基本与时间 $t$ 呈正相关。不妨从简单的函数入手，设 $g(t) = t^k$（先假定 $k \in \mathbb Z$），此时要拟合函数 $N(t)$ 只需拟合函数 $f(t) = N(t) \cdot t^k$。依次尝试 $k = 1, 2, \cdots$，当取 $k = 3$ 时，函数 $f(t)$ 如图 2 中蓝色曲线所示。易见其已经可以利用二次函数进行较为良好的拟合。使用最小二乘法拟合出的结果为
$$\tilde f(t) = 6851251.27x^2 + 308777521x - 497008077,$$

其图象如图 2 中橙色曲线所示。因此这给出对 $N(t)$ 的拟合为
$$\tilde N(t) = \frac{6851251.27}{x} + \frac{308777521}{x^2} - \frac{497008077}{x^3}.$$

从 $t = 50$ 开始，函数 $N(t)$ 与 $\tilde N(t)$ 的图象如图 3 所示（其中蓝色曲线为 $N(t)$，橙色曲线为 $\tilde N(t)$）。可以看出，$\tilde N(t)$ 已经是对 $N(t)$ 的一个较好的拟合，且 $\displaystyle\lim_{t \to \infty} N(t) = 0$ 的趋势也得到了保障。因此，我们选用 $\tilde N(t)$ 作为最终的拟合函数。

![](fig2.png "图 2")
![](fig3.png "图 3")

回顾我们最开始确定的 $f(t)$ 的意义，它表明了游戏在人群中的扩散程度。仅从这一点看，我们拟合出的二次函数不足以给出一个合理的解释，因为二次函数最终会趋于无穷，但扩散程度却会趋于一个饱和阈值。不过，这并不表明我们构建的函数模型是虚弱的，因为无论是它对已有数据的拟合效果还是它对未来趋势的预估都令人满意。我们应当认为 $N(t)$ 仅是来自于表达式 $\displaystyle\frac{f(t)}{g(t)}$ 的结果，而函数 $f(t)$ 与 $g(t)$ 本身则可以具有其它形式，毕竟，我们还强行规定了 2022 年 1 月 7 日对应 $t = 1$。

2023 年 3 月 1 日对应 $t = 419$，而 $\tilde N(419) \approx 18103$。从 $t = 200$ 开始，$N(t)$ 与 $\tilde{N}(t)$ 的最大误差 $\max \vert N(t) - \tilde N(t)\vert \approx 6321$。因此，我们给出 2023 年 3 月 1 日的游戏报告结果数区间

$$I_\text{p} = [18103 - 6321, 18103 + 6321] = [11782, 24424].$$

### 3.2 Attribute Analysis

对于问题“单词的属性是否会影响结果百分比”，我们的回答是“是”。

直觉上，我们认为当结果为越常用的单词时越容易尽早被猜出来——想象你正在亲自玩 Wordle 游戏，你通常会优先尝试最早出现在你脑海里的含有五个字母的单词，而这些单词往往是较为常见的。为此，我们需要一份描述各个单词在平时生活中或文章中出现频率的数据。幸运的是，有人帮助我们完成了这一项工作[^2]，这份数据整理自 Wolframe，将所有 Wordle 词库中的单词按频率映射到了 $[0, 1)$。

[^2]:[https://github.com/3b1b/videos/blob/master/_2022/wordle/data/freq_map.json](https://github.com/3b1b/videos/blob/master/_2022/wordle/data/freq_map.json)。

对于一个单词，设其频率权值为 $\rm Frq$，猜测次数平均值（大于 $6$ 次按 $7$ 次计算）为 $\rm Avg$，我们可以计算出如下两个属性 $y_1, y_2$[^3]：
$$\begin{aligned}y_1 &= 20 + \ln({\rm e}^{-20} + {\rm Frq}),\\ y_2 &= {\rm Avg} + 3.5.\end{aligned}$$

[^3]:在这里，计算 $y_1$ 时取 $\ln$ 的原因是频率指数差异较大。

上述运算保证了 $y_1, y_2$ 的结果相近，且不改变 $\rm Frq$ 与 $\rm Avg$ 的总体趋势，即我们可以认为 $y_1, y_2$ 分别直接代表了 $\rm Frq$ 与 $\rm Avg$。将所有单词按照 $y_1$ 从小到大排序后，$y_1, y_2$ 关于单词的图象如图 4 所示。

![](fig4.png "图 4")

尽管不明显，但我们仍可以发现 $y_2$ 随着 $y_1$ 的升高总体呈下降趋势，这表明单词常见与否确实会影响猜测的结果百分比。一些实际的例子也说明了这一点：对于常见的单词 $\tt dream$，有 $5\%$ 的人仅通过一次尝试就猜对，而仅有 $1\%$ 的人没有在 $6$ 次尝试内猜对；对于不常见的单词 $\tt parer$，几乎没有人仅通过一次尝试就猜对，而有约 $48\%$ 的人没有在 $6$ 次尝试内猜对。

相邻两个字母的组合可能也会影响猜测结果。例如，我们认为含有字母组合 $\tt ea$ 的单词更容易被猜中（如 $\tt dream, feast$，从给出的数据来看它们的猜测结果都较为良好），而含有字母组合 $\tt aw$ 的单词则不容易被猜中（如 $\tt gawky, awful$）。

其他一些因素也值得考虑，如一个单词内的元音字母数量、辅音字母数量以及不同字母的数量，尽管目前我们无法给出一个它们与结果百分比关联的显式结果。所有的这些因素都将用于 4.2 部分深度学习中单词特征向量的构建。

## 4 Task 2: Prediction of Result Percentages

利用单词的特征对 $7$ 项结果百分比做直接估计是较为困难的。在 4.1 部分中，基于结果百分比本身，我们首先将所有测试过的单词分为了若干类，满足每一类中的所有单词的结果百分比尽量接近。随后，我们在 4.2 部分中利用深度学习构建出了一个以单词特征作为输入，分类编号作为输出的预测模型，并以单词 $\tt eerie$ 为例说明了如何通过分类编号得到具体的结果百分比预测。在最后的 4.3 部分，我们对预测的信心与不确定因素做了分析。

### 4.1 Classification

首先确定分类数 $K$。$K$ 不应过小，否则每一类中单词的结果百分比将不会呈现出较为明显的共同特征；也不应过大，否则对深度学习过程不利。此处取 $K = 15$。

我们使用 $K$-means 聚类算法完成这一过程。对于每一个单词，设其结果百分比分别为 $p_1, \cdots, p_6, p_{\rm X}$，它们构成了 $7$ 维坐标向量 $\boldsymbol p = (p_1, \cdots, p_6, p_{\rm X})$。对 $7$ 维空间中的任意两个点 $P, Q$，定义它们的欧几里得距离为聚类算法的距离函数，即设两点坐标向量为 $\boldsymbol p = (p_1, \cdots, p_6, p_{\rm X}), \boldsymbol q = (q_1, \cdots, q_6, q_{\rm X})$，则它们之间的距离

$$d(P, Q) = \sqrt{(p_{\rm X} - q_{\rm X})^2 + \sum_{i=1}^6 (p_i - q_i)^2}.$$

选取该函数意味着被归为同一类的点，它们的各项结果百分比差距不会过大，这与我们的分类期望是相符合的。

关于利用 $K$-means 算法对所有单词进行分类的具体过程，由于与模型本身关联较小，因此这里略去。具体过程可在附录 I 中查看。

![](fig5.png "图 5")

在成功将所有单词分为 $K = 15$ 类后，我们通过两个典型的统计数据：平均值和方差，来展示我们的分类效果，如图 5 所示。可以看到，平均值和方差接近的单词被分类在了同一个聚类中，因此对于未来的单词，如果我们顺利地确定了它所在的类别，那么预测出它的各项结果百分比也就较为容易了。

### 4.2 Modeling by Neural Network

现在，我们已经拥有了给定单词的诸多特征以及主要依据结果百分比分出的 $15$ 个类别。我们希望据此构建一个模型满足以单词及其特征作为输入，类别作为输出，它能作用到整个结果词集上，来帮助我们对未来的单词进行预测，也即形式化地，我们需要构建一个分类函数

$$h: {\boldsymbol x} \in \mathbb R^{m} \to y \in \{0, 1, \cdots, 14\},$$

其中 $\boldsymbol x$ 是由单词本身的特征构成的一个 $m$ 维实列向量，$m$ 是特征的数量，$y$ 为类别编号。我们利用基于神经网络的深度学习实现这一函数的构建。

首先，利用已有的特征，我们可以为每一个单词构造出一个 $21$ 维的特征列向量，即 $m = 21$。列向量的具体构成如下（以 $\boldsymbol x_i$ 表示列向量的第 $i$ 维）：

- $\boldsymbol x_{0 \sim 4}$：依次代表单词本身的各个字母。字母 $\tt a, b, \cdots, \tt z$ 对应的实数依次为 $\displaystyle\frac{0}{26}, \frac{1}{26}, \cdots, \frac{25}{26}$。[^4]
- $\boldsymbol x_{5 \sim 9}$：依次代表单词各个字母在词库中的出现频率。
- $\boldsymbol x_{10 \sim 13}$：依次代表相邻两个字母的组合。这里使用进制的思想完成实数对应，例如字母 $\tt b, c$ 分别对应 $1, 2$，因而字母组合 $\tt bc$ 对应实数 $\displaystyle\frac{1 \times 26 + 2}{26^2}$。
- $\boldsymbol x_{14 \sim 16}$：依次代表相邻三个字母的组合。对应方式同上。
- $\boldsymbol x_{17}$：元音字母数量除以 $5$。
- $\boldsymbol x_{18}$：辅音字母数量除以 $5$。
- $\boldsymbol x_{19}$：不同字母数量除以 $5$。
- $\boldsymbol x_{20}$：单词使用频率。

[^4]:这里所有数字除以 $26$ 是刻意为之。事实上，限制特征列向量 $\boldsymbol x \in [0, 1]^m$ 在深度学习中十分重要，它既能防止运算溢出，又能避免激活函数的斜率过小从而降低学习效果。

接下来构建神经网络。初始特征向量维度 $m = 21$ 与最终输出层的节点数量 $15$（即分类数）已经对中间层的节点数量做了较强的限制，因此这里我们仅构建双层神经网络，它由一个包含 $20$ 个节点的隐藏层和 $15$ 个节点的输出层组成，如图 6 所示。

![](fig6.png "图 6")

关于利用神经网络进行学习与预测的具体过程，由于与模型本身关联较小，因此这里略去。具体过程可在附录 II 中查看。

最后，在经过较多次学习迭代后，神经网络在题目文件已经给出的单词集上的输出准确率约为 $61\%$。这一准确率看似不高，但实际上单词本身可供提取的特征数并不多，这导致初始特征向量维度与最终输出层的节点数量之差极为微小（通常，利用神经网络进行深度学习的初始特征向量维度至少在千级甚至万级以上），特征的相关性也并不明显，因此取得大于 $60\%$ 的准确率已经是令人满意的结果。同时，若准确率过高也会出现过拟合现象，这不利于对未来单词的预测。

神经网络对 2023 年 3 月 1 日的结果单词 $\tt eerie$ 给出的预测分类编号是 $2$。该类中单词的结果百分比分布较为均匀，因此直接取平均值作为预测结果，用七元组表示即为：

$$(p_1, p_2, p_3, p_4, p_5, p_6, p_{\rm X}) = (0\%, 2\%, 11\%, 28\%, 32\%, 21\%, 5\%).$$

### 4.3 Model Analysis

模型的不确定性主要来自于两个方面，它们与上述分类与深度学习两个步骤分别对应。

在分类上，由于所有单词的结果百分比分布没有明确界限，因此处于不同分类中的单词结果差距并非一定明显，两个距离极小的点被分到了不同的类中的情况完全可能出现。同时，$K$-means 算法在一定程度上基于随机化，这也引入了不确定因素，例如，类中可能包含这样的点，它与类中其他点相距很远，这会给该类对应的结果预测带来一定的麻烦。

在深度学习上，不确定性也会由随机的初始参数引入。有时，一个不当的初始参数容易使学习陷入局部最优结果（而非全局最优结果），这意味着需要通过多次学习来寻找较为良好的参数。同时，略大于 $60\%$ 的已有单词预测准确率尽管理论上已经不错，但假设你对深度学习了解甚少，这一数值似乎并不会使你信服。

不过，尽管模型具有不确定性，我们对于单词 $\tt eerie$ 的预测却较有信心。一方面，在连续的 $5$ 次学习中，神经网络有 $3$ 次对于单词 $\tt eerie$ 给出的预测类别编号都是 $2$，这表明对于单词 $\tt eerie$，从特征向量到分类编号的映射较为明显和稳定；另一方面，这一结果在经验上也较为正确——$\tt eerie$ 通常不会作为一个玩家在首次猜测中使用的单词，因此预测的 $p_1 = 0$，同时，该单词在使用频率中处于中间水平，且含有三个元音字母 $\tt e$ 的特征较为特殊，因此认为最后才猜中的概率 $p_6 = 21\%$ 以及猜不出的概率 $p_{\rm X} = 5\%$ 也比较合理。

## 5 Task 3: Difficulty Modeling

### 5.1 Difficulty Classification

伴随着大量与信息熵有关的 Wordle 游戏理论介绍的出现[^5]，仅根据传统的随机策略或者一些较为依靠主观灵感的策略来评估 Wordle 游戏中单词难度的做法已不可取。在对单词难度进行具体分类前，我们首先对信息熵理论做一个简述。

[^5]:如 [https://youtu.be/v68zYyaEmEA](https://youtu.be/v68zYyaEmEA)。

为了方便，首先假定所有可填单词均可作为某一轮 Wordle 游戏的答案，且各个单词作为答案的概率相等。在一轮 Wordle 游戏中，设某一时刻可能成为答案的单词集合为 $S$，玩家进行一次单词猜测后，根据游戏给出的反馈，$S$ 将会缩小为它的一个子集 $S'$。设在所有可能的 $k$ 种反馈中 $S$ 缩小成的子集分别为 $S_1, S_2, \cdots, S_k$，由于我们已假定 $S$ 中的每个词作为答案出现的概率相等，因此 $S$ 缩小为子集 $S_i$ 的概率

$$P(S \to S_i)=\frac{|S_i|}{|S|},$$

这里 $|S|$ 表示集合 $S$ 的大小。此时，为了让一次猜测的价值更高，我们希望在最坏情况下进行的尝试次数尽量少，这意味着 $S_1, \cdots, S_k$ 出现的概率基本均等，即 $\vert S_i \vert$ 尽量接近，同时 $k$ 尽可能大。为此，对于当前可能的答案集合 $S$ 以及猜测词 $s$，我们引入信息熵（将 $P(S \to S_i)$ 简记为 $P(S_i)$）

$$\text{Entropy}(S, s) = -\sum_{i = 1}^k P(S_i)\cdot \log P(S_i).$$

可以发现，$S_i$ 出现的概率越均等，$k$ 越大，熵 $\text{Entropy}(S, s)$ 就越大。

这样，我们就有了一种评估某一次猜测优劣的方法，也得到了一种较为优秀的 Wordle 游戏策略：每一次，我们对所有可猜测单词 $s$ 计算 $\text{Entropy}(S, s)$，选取 $\displaystyle\argmax_s\{\text{Entropy}(S, s)\}$ 作为该轮的猜测单词。

然而，在实际的 Wordle 游戏中，每个词作为答案的概率并不相同，在频率表（见 3.2 部分）中权值较低的单词通常不会成为答案词。[^6]我们不妨为每个单词加权，单词 $s$ 的权重

$$W(s) = \ln({\rm Frq}(s)) + C,$$

其中 ${\rm Frq}(s)$ 为单词 $s$ 的词频，$C$ 为用于调整权重的常数。这样，在某一时刻 $S$ 缩小为子集 $S_i$ 的概率也会发生改变，具体地，

$$P(S \to S_i)=\cfrac{\sum_{s\in S_i}W(s)}{\sum_{s\in S}W(s)}.$$

[^6]:事实上，Wordle 游戏有一个专门的可作为答案词的单词库。但在这里，假设我们并不知道这一词库的存在。

有了上述理论，我们来构建具体的评估 Wordle 单词难度的模型。

基于经验，我们将所有 Wordle 玩家分成以下三类：

- 严格使用上述理论中的优秀策略进行游戏的玩家。
- 近似使用上述理论中的优秀策略进行游戏的玩家。这类玩家在进行游戏时，每一步会在 $\text{Entropy}(S, s)$ 最大的前 $5\%$ 的单词 $s$ 中随机选择一个词进行猜测。
- 始终使用随机策略进行游戏的玩家。

当然，后两类玩家的“随机”猜词策略并非完全等概率随机，较为贴切的假设是考虑词频。具体而言，当他们随机选择单词时，选择词 $s$ 的概率权重为 ${\rm Frq}(s)$。

我们希望知道这三类玩家的大致占比，设为 $a, b, c$。为此，在设定好三类玩家的初始比例 $(a, b, c) = (a_0, b_0, c_0)$ 后，我们需要利用已给出的单词及其结果进行梯度下降过程来找到 $a, b, c$ 的最优值。具体地，通过多次随机模拟可以计算出在当前玩家比例 $(a, b, c)$ 下猜测结果百分比 $(\tilde p_1, \cdots, \tilde p_6, \tilde p_{\rm X})$，而实际的结果百分比 $(p_1, \cdots, p_6, p_{\rm X})$ 就是 $(\tilde p_1, \cdots, \tilde p_6, \tilde p_{\rm X})$ 需要拟合的对象。这也就自然给出了梯度下降法的代价函数

$$J(a, b, c) = \sqrt{(\tilde p_{\rm X} - p_{\rm X})^2 + \sum_{i = 1}^6(\tilde p_i - p_i)^2}.$$

梯度下降法的具体流程仍可见附录 II，这里略去。我们最终得到的最优拟合结果对应

$$(a, b, c) = (1.407\%, 56.149\%, 42.444\%).$$

基于这一数据，对于单词 $s$，我们可以用整个玩家群体的期望猜测次数来确定难度。具体地，设第一类玩家所需的猜测次数为 $T_1$，第二类玩家所需的期望猜测次数为 $T_2$，第三类玩家所需的期望猜测次数为 $T_3$，则单词 $s$ 的难度定义为：

$$\text{Lvl}(s) = t(aT_1 + bT_2 + cT_3),$$

其中

$$t(x) = \begin{cases}1, &0 \leq x \leq 4.5, \\ 2, &4.5 < x \leq 5, \\ 3, &x > 5.\end{cases}$$

容易发现，我们通过对群体的期望猜测次数设置分界点 $4.5$ 与 $5$ 将所有单词分为了三种难度，这里 $1, 2, 3$ 分别对应的难度为简单、中等与困难。结合任务 2 中建立的神经网络，我们可以在确定预测单词所属的类别后，根据类中单词的整体难度确定预测单词的难度。

神经网络对单词 $\tt eerie$ 给出的预测分类编号为 $2$，该类单词对应难度函数 $\rm Lvl$ 的值为 $2$，因此我们预测 $\tt eerie$ 的难度等级为中等。

### 5.2 Model Analysis

在我们的模型中，由于单词的难度直接由其所在的类决定，因此影响单词难度的因素几乎完全受限于影响其所在类的因素。下面仅对难度模型本身的合理性与准确性进行分析。

合理性方面，对于任意一款游戏来说，作为其受众的玩家群体都对游戏难度的评估有绝对的发言权，Wordle 游戏也不例外，玩家的猜测次数几乎是评估单词难度的唯一准则。我们自然猜想，单词难度应当与所有人猜测次数的平均值有关，且平均值越大，难度就越高。同时，在我们确定的参数下，会使用优秀策略的第一、二类玩家与仅使用随机策略的第三类玩家的比例为 $57.556\% : 42.444\%$，近似为 $1:1$，选取平均数对各类群体都具有一定的代表意义。

准确性方面，每一类中单词难度的分布如图 7 所示（图中横坐标为类编号，纵坐标为整个玩家群体的期望猜测次数）。经过计算，仅有第 $9$ 类单词的猜测次数方差约为 $0.04$，除此之外，大多数类的方差均在 $0.02$ 附近。可以看出，除第 $9$ 类单词，其余类单词在函数 $t$ 的作用下基本都能得到较为统一的难度等级，而第 $9$ 类单词的期望猜测次数也主要集中在区间 $(4.5, 5]$ 中，我们有理由用中等难度评估该类单词。

![](fig7.png "图 7")

上述该难度模型的合理性与准确性为其准确度提供了保障。

## 6 Task 4: More Data Features

通过数据集，我们注意到单词被一次猜中的概率分布随时间变化很大。在早期（2022 年 4 月以前），每天的单词几乎都能有至少 $1\%$ 的人能一次猜中，而到了后期，只有少数常见词和信息熵较高的词语能有至少 $1\%$ 的人一次猜中。

这一变化与玩家的游戏策略密切相关。在早期，大部分新玩家面对 Wordle 没有较为明智的策略，“灵光一闪”式的随机单词成为了他们填入游戏的第一个单词，而其中总有一部分玩家足够幸运，他们仅通过一次尝试便猜对了结果；到了后期，随着新玩家的减少以及游戏策略在互联网上的传播与普及，玩家逐渐形成较为统一的游戏策略，他们总是从信息熵高的单词开始，因而几乎总是不能一次猜对，除非结果单词本身就是一个信息熵高的单词（例如 2022 年 12 月 19 日的单词 $\tt slate$）。

我们还注意到选择困难模式的玩家占比随天数不断上升，且上升趋势在年末已经放缓，目前参与困难模式的玩家比率稳定在 $10\%$ 附近，如图 8 所示。

![](fig8.png "图 8")

最后，给出的数据集中存在着一些明显错误的数据，这在某种程度上也是一种“有趣的特征”。例如，部分单词出现了多字母或少字母现象，部分数据出现了缺位现象。在实际处理中，我们都解决或者避开了这些数据问题。

## 7 Sensitivity Analysis

### 7.1 Reported Result Model

对天数和报告结果数建立的模型 $N(t)$ 直接受到分母函数 $g(t)$ 的影响。

在最开始的假定中，我们构造 $g(t) = t^k$，其中 $k \in \mathbb Z$。随后经过尝试，选择了 $3$ 作为 $k$ 的最终值。事实上，当 $k = 1$ 和 $2$ 时，$f(t) = N(t) \cdot t^k$ 的图象分别如图 9 和图 10 所示。

![](fig9.png "图 9")
![](fig10.png "图 10")

可以看出，在 $k = 1$ 和 $2$ 的情况下，分子函数 $f(t)$ 仅仅满足单调递增，其较为直观的初等函数拟合很难给出。事实上，我们还有更多其他函数可供尝试，例如 $g(t) = t^r, r \in \mathbb R$，或者直接考虑用形如 $\displaystyle\frac{x}{{\rm e}^x}$ 的函数来拟合 $N(t)$，因为该函数的图象与图 1 所示的 $N(t)$ 的图象大致相同。选择 $g(t) = t^3$ 从某种程度上说仅仅是一种巧合。这些分析都表明模型对分母函数 $g(t)$ 的多项式指数甚至形式的选择是十分敏感的，尽管它的预测效果尚可。

### 7.2 Result Percentage Classification and Prediction Model

结果百分比的分类与预测模型总体较为稳定。

对于分类模型，无论所有单词的结果百分比分布如何，在给定分类数量 $K$ 的情况下，执行聚类算法总是能给出可以让人接受的结果，除非出现多点重合这样过于极端的情况。敏感性仅仅在于分类数 $K$ 的选择上。事实上，在 4.1 部分我们已经对其进行了说明：$K$ 过小会导致每一类单词的特征不明显，$K$ 过大会对深度学习过程不利。

对于基于深度学习的预测模型，尽管它强依赖于单词频率等特征与单词类别之间的关联程度，但只要这一关联存在，我们总能通过多次学习取最优参数来强行将关联凸显出来。换句话说，多次尝试取最优解这一步骤可以尽可能地掩盖模型对其他因素的敏感性。这也正是深度学习的优势所在。

### 7.3 Difficulty Model

对于单词难度模型，影响模型的参数主要是三类玩家的占比 $(a, b, c)$。

在使用梯度下降法时，假设对参数 $a$ 进行了调整，由于要求 $a + b + c = 1$，因此剩余两类玩家的占比 $b, c$ 将会保持比例进行调整。例如，设 $(a, b, c) = (0.2, 0.3, 0.5)$，欲调整第一类玩家占比至 $0.6$，则调整后有 $(a, b, c) = (0.6, 0.15, 0.25)$，注意调整前后始终有 $b:c = 3:5$。

表 1、表 2 与表 3 分别给出了在 $(a, b, c) = (1.407\%, 56.149\%, 42.444\%)$ 的基础上，第一、二、三类玩家占比发生变化时在测试集上的损失函数、词语的平均难度与词语难度方差的变化。

| 第一类玩家占比 | 损失函数 | 平均难度 | 难度方差 |
| ------------ | -------- | -------- | -------- |
| $1.41\%$        | $0.186699$ | $4.38608$  | $0.104138$ |
| $2.80\%$        | $0.189401$ | $4.37212$  | $0.104316$ |
| $0.70\%$        | $0.187147$ | $4.39255$  | $0.104054$ |

| 第二类玩家占比 | 损失函数 | 平均难度 | 难度方差 |
| ------------ | -------- | -------- | -------- |
| $56.14\%$       | $0.186699$ | $4.38608$  | $0.104138$ |
| $70.00\%$       | $0.211853$ | $4.24092$  | $0.098189$ |
| $40.00\%$       | $0.192222$ | $4.55199$  | $0.112854$ |

| 第三类玩家占比 | 损失函数 | 平均难度 | 难度方差 |
| ------------ | -------- | -------- | -------- |
| $42.44\%$       | $0.186699$ | $4.38608$  | $0.104138$ |
| $60.00\%$       | $0.19346$  | $4.57273$  | $0.114087$ |
| $20.00\%$       | $0.233504$ | $4.15796$  | $0.095493$ |

从表格中可以看出，无论哪一类玩家占比上升或下降均会引起损失函数上升。尽管看上去改变第二、三类玩家占比对模型的影响程度大于改变第一类玩家占比对模型的影响程度，但对第二、三类玩家占比改变的幅度本身也较大（在表 1 中，第一类玩家占比仅改变了 $+1.39\%$ 与 $-0.71\%$）。总的来说，模型对参数 $(a, b, c)$ 并不敏感。

## 8 Strengths and Weeknesses

## 9 Conclusion

在这篇文章中，我们基于已经给出的 Wordle 报告数据集构建了一个完整的模型，它由分别负责每日结果报告数预测、单词分类、结果百分比预测与难度评估等功能的子模型构成。这些功能中，每日结果报告数预测相对独立，它基于传统的函数回归拟合，除此之外的功能的实现则基于较为现代的技术，如聚类算法、梯度下降与深度学习。这些功能总体上层层递进，共同完成了该问题下的所有任务。负责结果百分比预测与难度评估的模型尽管看上去复杂，但各个步骤环环相扣，现代技术的引入也十分自然。此外，我们还对数据集本身做了进一步分析。

从对单词 $\tt eerie$ 的预测结果来看，我们的模型具有良好的准确性与稳定性，因为预测结果与经验判断较为符合，且输出稳定。模型在敏感性分析中的表现也不错。总体来说，我们的模型的工作效果令人满意。当然，模型也不乏一些缺陷，受制于较短促的时间，我们尚不能立刻给出优化它们的良好方法。我们希望这些问题能在未来对 Wordle 游戏本身及其数据的研究工作中得到解决。

## 10 A Letter to the Puzzle Editor of the New York Times

## 11 References

## Appendix I: Process of Cluster Algorithm

首先，我们需要确定 $K$ 个初始聚类的中心。确定步骤如下：

1. 随机选择一个单词坐标点作为第一个聚类的中心，记为 $P_1$。
2. 假设当前已经确定的聚类中心有 $K'$ 个，我们对于每个单词坐标点 $P$，计算：$d = \displaystyle \sum_{i = 1}^{K'} d(P_i, P)$，并对所有单词坐标点求和 $D = \displaystyle\sum_P d$。
3. 对每个单词坐标点 $P$，计算 $p(P) = \displaystyle\frac{d}{D}$ 表示选择 $P$ 作为第 $K' + 1$ 个聚类中心的概率。容易验证 $\displaystyle\sum_P p(P) = 1$。根据概率选择下一个聚类的中心节点 $P_{K' + 1}$。
4. 如果此时 $K' = K$，则整个过程结束，否则返回步骤 2。

接下来进行若干轮迭代。每一轮迭代如下：

- 对于每一个单词坐标点 $P$，我们选择 $K$ 个聚类中心中 $d(P, P_i)$ 最小的中心节点 $P_i$，并把 $P$ 归入第 $i$ 个聚类。
- 对于第 $i$ 个聚类，找出其中到属于该聚类的其他点的距离之和最小的点，将其设为新的第 $i$ 个聚类的中心 $P_i$。

在实际实现时，我们迭代了 $100$ 轮后分类结果就已经接近收敛。迭代结束后，所有单词便按照结果百分比被成功分为了 $K = 15$ 类。最终分类结果可见 [???.com](www.baidu.com)。

## Appendix II: Process of Deep Learning

对于图 8 的神经网络，隐藏层和输出层的每一个节点事实上还包含了如下信息：一个权重列向量 $\boldsymbol w$，一个偏移值 $b$ 以及一个激活函数。其中，隐藏层的权重向量 $\boldsymbol w \in \mathbb R^{21}$，激活函数为 ReLU（the Rectified Linear Unit）函数：

$${\rm ReLU}(z) = \max\{0, z\},$$

输出层的权重向量 $\boldsymbol w \in \mathbb R^{20}$，激活函数为 Sigmoid 函数：

$$\sigma(z) = \frac{1}{1 + {\rm e}^{-z}} \in (0, 1).$$

对于一个给定的单词向量 $\boldsymbol x \in \mathbb R^{21}$，通过神经网络计算其分类的过程如下：

- 对于隐藏层的每一个节点，计算 $a = {\rm ReLU}(\boldsymbol w^{\rm T}\boldsymbol x + b)$，这里 $\boldsymbol w$ 与 $b$ 即为节点自身的权重向量与偏移值，$\boldsymbol w^{\rm T}$ 表示对向量 $\boldsymbol w$ 取转置。
- 将隐藏层所有节点的计算值 $a$ 组合为一个新的列向量 $\boldsymbol a \in \mathbb R^{20}$。
- 对于输出层的每一个节点，计算 $\tilde a = \sigma(\boldsymbol w^{\rm T}x + b)$，这里 $\boldsymbol w$ 与 $b$ 即为节点自身的权重向量与偏移值。
- 取输出层中 $\tilde a$ 最大的节点编号作为最终结果。

每个节点的 $\boldsymbol w$ 与 $b$ 初始时需进行随机赋值。然而，直接利用初值来对单词进行预测，得到的仅仅是一个随机的结果。我们需要利用已有的单词数据与分类来调整各个节点的 $\boldsymbol w$ 与 $b$，即进行所谓的学习过程。对于一次输入，我们期望神经网络仅在输出层的正确编号（即单词所在类的编号）对应的节点输出 $1$，而在其它节点输出 $0$。为此，可以定义一个与输出向量 $\tilde{\boldsymbol a} \in \mathbb R^{15}$ 与结果向量 $\boldsymbol y \in \mathbb R^{15}$ 有关的凸性函数 $\mathcal L(\tilde{\boldsymbol a}, \boldsymbol y)$ 来作为评估标准，这里

$$\mathcal L(\tilde{\boldsymbol a}, \boldsymbol y) = -\sum_{i = 0}^{15} \boldsymbol y_i\cdot \log \tilde{\boldsymbol a}_i + (1 - \boldsymbol y_i)\cdot \log (1 - \tilde{\boldsymbol a}_i).$$

当输出层的结果越接近于我们期待的结果时，函数 $\mathcal L(\tilde{\boldsymbol a}, \boldsymbol y)$ 的值将会越小。对于所有 $n$ 个已提供的单词，我们可以通过代价函数

$$J(\{\tilde{\boldsymbol a}, \boldsymbol y\}) = \frac{1}{n}\sum \mathcal L(\tilde{\boldsymbol a}, \boldsymbol y).$$

来评估神经网络当前总体的表现情况。这样，学习过程的目的便是使代价函数 $J(\{\tilde{\boldsymbol a}, \boldsymbol y\})$ 尽可能小。

利用好评估函数具有凸性的特征，学习过程采用梯度下降法，单次学习迭代过程如下：

- 对于所有 $n$ 个已提供的单词，利用神经网络计算代价函数 $J(\{\tilde{\boldsymbol a}, \boldsymbol y\})$。
- 注意到 $\tilde{\boldsymbol a}$ 本质上是由初始特征以及所有节点的 $\boldsymbol w$ 与 $b$ 计算得到，因此可以对每个节点计算偏导数或偏导向量 $\displaystyle \frac{\partial J}{{\partial}{\boldsymbol{w}}}, \frac{{\partial}J}{{\partial}{b}}$。
- 每个节点的 $\boldsymbol w$ 被更新为 $\boldsymbol w - \alpha\displaystyle \frac{{\partial}J}{{\partial}{\boldsymbol{w}}}$，$b$ 被更新为 $b - \alpha\displaystyle\frac{{\partial}J}{{\partial}{b}}$，其中 $\alpha$ 表示每次朝着梯度方向调整的幅度（在深度学习中，它被称为“学习率”），为预先指定的一个参数。例如，可以取 $\alpha = 0.05$。

经过若干次迭代训练（我们实际进行了 $10^6$ 次）后，若神经网络在已有单词数据上表现良好，我们就可以用其来对未来单词进行预测。用于预测的最终参数见 [???.com](www.baidu.com)。